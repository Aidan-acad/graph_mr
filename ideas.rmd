---
title: Constructing deconvolved causal graphs from GWAS summary data
author: G Hemani et al
date: "`r Sys.Date()`"
output: 
  pdf_document:
    keep_tex: true
bibliography: ideas.bib
csl: nature.csl
---


```{r, echo=FALSE, cache=FALSE}

# http://www.mathpages.com/home/kmath198/2-1/2-1.htm
# http://videolectures.net/gpip06_mackay_gpb/

suppressPackageStartupMessages(library(knitr))

read_chunk("functions.r")
source("functions.r")
opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE)
set.seed(1234)

```

**Code available at:** [https://scmv-ieugit.epi.bris.ac.uk/gh13047/graph_mr](https://scmv-ieugit.epi.bris.ac.uk/gh13047/graph_mr)

## Summary

Suppose we have complete summary statistics for an arbitrary number of traits, and each trait has valid instruments. Mendelian randomisation can be used to calculate the causal relationships of $M \times M$ pairs of traits, thereby constructing a matrix of *total* effects of each trait on each of the other traits. This paper introduces a method to deconvolve this matrix to obtain the set of all direct causal effects between the traits. It goes on to show that the statistical power of identifying causal relationships between traits improves substantially by chaining together the direct effects on the deconvolved pathway, when compared to evaluating the total effect as is typical in Mendelian randomisation.


## Introduction


Many methods exist that attempt to decompose correlation matrices into a set of terse direct correlations, with a view towards obtaining the minimum set of correlations that can explain the observed matrix. This process is known as deconvolution [@Feizi2013] [describe other methods in more detail]. A typical use case in biology is to calculate the correlation matrix of gene expression levels, and deconvolve to identify genes that drive networks. The application of matrix deconvolution to the context of causal inference in Mendelian randomisation (MR), as shown in Figre 1, could have the following attractive features:

- Identify direct pathways through which a particular exposure influences an outcome
- Identify instances of partial mediation, which might suggest that there are unknown variables that remain to be uncovered that mediate the path from exposure to outcome
- Potentially improve power in identifying causal relationships between traits that are on the same causal pathway

Growth in available GWAS summary data on phenotypes is on a steep trajectory, such that we may soon be asymptoting towards a situation where we can use two-sample MR to test 'everything against everything'. What this entails is that we could construct a pairwise causal relationship of all (available) traits (e.g. see Figure 1a,b). Each element in such a matrix would represent the *total* causal effect between the two traits. The purpose of this paper is to explore how to deconvolve this set of *total* effects into a terse set of *direct* effects. 

Attempts at constructing networks using MR have been proposed [@Burgess2015], based on the idea of mediation by MR [@Relton2012], though implementation of this method to more than three variables has not been demonstrated. While existing deconvolution methods can be applied to an arbitrary number of traits, they tend to require that the initial matrix is symmetrical, and they often do not incorporate information that disentangles correlation from causation. The setting for this paper assumes the contrary in both counts. The matrix of total MR effects is asymmetrical because the estimate of A on B (instrumented by SNPs that relate to A) is not the same as the estimate of B on A (instrumented by SNPs that relate to B). Further, each element in the matrix of total effects is assumed to be a causal estimate.


![Deconvolving causal estimates from raw MR estimates. a) For each trait that can be instrumented, perform MR of its effect against every other trait. e.g. If there are six traits then there are 6^2 - 6 = 30 MR estimates to generate. b) Use these results to construct the matrix of total effects. c) Use a deconvolution method to obtain a set of direct causal effects. d and e) The true causal model is shown in (e), but if the raw causal effects are estimated using MR (i.e. the *total* effects) then a number of extra paths will be identified owing to transitive effects of the direct paths.](schematic-01.png)


## Methods


### Assumptions

1. It is important to acknowledge that, though beyond the scope of this paper, the premise of obtaining the causal effect of 'everything on everything' requires methodological advancements to address many issues, including but not limited to multiple testing, consideration of phenotypic definitions, and incorporating temporal effects and critical effect periods.

2. We begin by assuming that the causal effect estimates of each element in the *total* effects matrix is unbiased (Figure 1a,b). In practice it is not yet clear how to ensure this, though methods are continually developing to improve the reliability of MR estimates to violations of its main assumptions [@Bowden2015; @Bowden2016a; @Bowden2016b; @Hemani2016].


### Direct and total effects

The latter can be summarised as follows. Suppose there are six variables of interest, 1-6, and the causal relationships are

```
1 -> 2
2 -> 3
3 -> 4
4 -> 5
5 -> 6
```

This can be depicted in graph form as in Figure 1e. If, however, we performed MR of `1 -> 3`, `1 -> 4`, etc, we would identify associations because they exist indirectly. Hence, after testing everything against everything our graph would look like Figure 1d.

MR for mediation (AKA network MR) operates in the case where there are three phenotypic variables [@Burgess2015a] as follows. The direct effect of trait 1 on trait 2, $\beta_{1 \Rightarrow 2}$, is obtained from:

$$
\beta_{1 \Rightarrow 2} = \beta_{1 \rightarrow 2} - \beta_{1 \rightarrow 3}\beta_{3 \rightarrow 2}
$$

With four variables it looks like:

$$
\begin{aligned}
\beta_{1 \Rightarrow 2} = \beta_{1 \rightarrow 2} & - \beta_{1 \rightarrow 3}\beta_{3 \rightarrow 4}\beta_{4 \rightarrow 2} \\
& - \beta_{1 \rightarrow 3} \beta_{3 \rightarrow 2} \\
& - \beta_{1 \rightarrow 4} \beta_{4 \rightarrow 2}
\end{aligned}
$$

With five variables it looks like:

$$
\begin{aligned}
\beta_{1 \Rightarrow 2} = \beta_{1 \rightarrow 2} & - \beta_{1 \rightarrow 3}\beta_{3 \rightarrow 4}\beta_{4 \rightarrow 5}\beta_{5 \rightarrow 2} \\
& - \beta_{1 \rightarrow 3} \beta_{3 \rightarrow 4} \beta_{4 \rightarrow 2} \\
& - \beta_{1 \rightarrow 3} \beta_{3 \rightarrow 5} \beta_{5 \rightarrow 2} \\
& - \beta_{1 \rightarrow 4} \beta_{4 \rightarrow 5} \beta_{5 \rightarrow 2} \\
& - \beta_{1 \rightarrow 3} \beta_{3 \rightarrow 5} \beta_{4 \rightarrow 2} \\
& - \beta_{1 \rightarrow 3} \beta_{3 \rightarrow 2} \\
& - \beta_{1 \rightarrow 4} \beta_{4 \rightarrow 2} \\
& - \beta_{1 \rightarrow 5} \beta_{5 \rightarrow 2}
\end{aligned}
$$

and this is performed for each of the $5 \times 5$ possible pairwise combinations of variables, ultimately reducing a matrix of total effect relationships, $R_{t}$ e.g.



```{r, fig.cap="Simulated causal relationships"}

dat1 <- init_dat(500000, 5)
dat1 <- make_edge(1, 2, -1, dat1)
dat1 <- make_edge(2, 3, -2, dat1)
dat1 <- make_edge(3, 4, 1, dat1)
dat1 <- make_edge(4, 5, 3, dat1)

res1 <- graph_mr(dat1)
round(res1$b, 1)
```

into a matrix of direct relationships, $R_{d}$ e.g.

```{r }
dat1$r
```

There are two potential drawbacks with the MR for mediation approach. First, the combinatorial increase in the number of terms that are required for calculating the direct effects gets large very quickly. For example, for a graph with 10 variables there are 52 unique paths for each of the 100 elements in the matrix, and identifying those paths itself is a computationally slow process. Second, perhaps more importantly, this method may not actually generalise beyond the three-trait case.

\newpage


## Simulations

Assume that there are $M$ variables measured in $N$ samples represented in a $N \times M$ matrix $P$. Further, each variable has a valid instrument, hence there are $M$ instruments also, represented in a $N \times M$ matrix $G$. As stated above, in this analysis I am assuming that every causal estimate made by MR is reliable.

DAGs are simulated such that the $M$ variables are related to each other by random causal effects. Cycles are avoided. Following on, two-stage least squares is used to calculate all pairwise causal relationships e.g.

$$
R_{t}(1 \rightarrow 2) = \frac{cov(P_{,2}, G_{,1} cov(P_{,1}, G_{,1} / var(G_{,1})))} { var(G_{,1} cov(P_{,1}, G_{,1} / var(G_{,1}))) }	
$$

Three methods are then used to try to deconvolve the graph $R_{t}$ into $R_{d}$.

### Method 1 - mediation by MR

This is as described earlier, adapting from [Relton2012] and [Burgess2015a].


### Method 2 - inversion

Simply a method to orthogonalise the matrix by 

$$
R_{d} = R_{t}^{-1}
$$

If $R_{t}$ were a variance covariance matrix then this method is known as obtaining the precision matrix.

### Method 3 - Feizi deconvolution

In [@Feizi2013] a method is outlined for network deconvolution that is primarily aimed at correlation matrices (i.e. symmetric, non-causal versions of $R_{t}$). The method is:

$$
R_{d} = R_{t}(I + R_{t})^{-1}
$$

### Standard errors

Bootstrapping is used to obtain the standard errors of the direct effects for the inversion method. Each element in $R_{t}$ is resampled with $R_{t}(i,j)* \sim N(R_{t}(i,j), se(R_{t}(i,j)))$. The inversion method is then applied to the resampled matrix $R_{t}*$ and the results $R_{d}*$ are stored. This is performed 1000 times to obtain a distribution of effects for each element of the $R_{d}$ matrix. The standard deviation of the distribution from each element is taken to be the standard error of that direct effect estimate. 


\newpage

## Results

The analysis is divided into two sections. First, a demonstration that in most cases the inversion method is rapid and returns the true direct effects; and second, an evaluation of statistical power, particularly in comparing the ability to detect a causal relationships via the total effect vs constructing a chain of intermediary effects.


### Comparison of deconvolution methods

Here will be presented analysis of four different causal networks. In all cases, 500000 samples are simulated to have the requisite phenotypes, and each phenotype is simulated to have a valid instrument. These data are used to construct the matrix of *total* causal effects, as shown in Figure 1a,b. The network is then deconvolved each of the three methods into $R_{d}$, and the estimates of the direct causal estimates are compared to the true direct estimates that were simulated initially.


#### Illustrative example


In each of the following cases large samples (n=500000) are simulated and causal effect sizes are large, to ensure that power is not influencing the fidelity of matrix deconvolution by the different methods. An illustrative example with the causal structure between four traits is shown in Figure 2. In this simple case, all methods resolve the same graph, however only the mediation and inversion methods obtain good agreement between the deconvolved direct effect estimates and the simulated direct effects (Figure 3).

The Feizi method appears to generate estimates proportional to the raw total effects, but on a scale that is shrunk towards 0.


```{r }

dat <- init_dat(500000, 4)
dat <- make_edge(1,2,2, dat)
dat <- make_edge(2,3,3, dat)
dat <- make_edge(1,3,7, dat)
dat <- make_edge(1,4,-2, dat)
dat <- make_edge(2,4,-4, dat)

res <- graph_mr(dat)
res1 <- mediation_method(res)
res2 <- inversion_method(res)
res3 <- deconvolution_method(res)
resall <- rbind(
	data.frame(estimate=c(res1), truth=c(dat$r), method="Mediation"),
	data.frame(estimate=c(res2), truth=c(dat$r), method="Inversion"),
	data.frame(estimate=c(res3), truth=c(dat$r), method="Feizi"),
	data.frame(estimate=c(res$b), truth=c(dat$r), method="Total effects")
)

```


```{r, fig.cap="Graphs depicting the simulated (true) causal graph, and the results from the different methods of estimation" }

par(mfrow=c(2,3))
plot_from_matrix(dat$r, "True graph")
plot_from_matrix(res$b, "Total effects")
plot(1, type="n", axes=F, xlab="", ylab="")
plot_from_matrix(res1, "Mediation method")
plot_from_matrix(res2, "Inversion method")
plot_from_matrix(res3, "Feizi method")

```

```{r, fig.cap="Comparison of deconvolved causal effect estimates against the simulated causal effects, using the elements of the Rd and Rt matrices."}


ggplot(resall, aes(y=estimate, x=truth)) +
geom_point(alpha=0.4) +
facet_wrap(~ method, scale="free") +
labs(x="True (simulated) direct effects", y="Estimated effects for each method")

```


#### Further simulations

Further simulations of more complex graphs are now performed. Beyond 7 variables the mediation method is computationally intractible using the algorithm as implemented here, but regardless it can be seen that any models more complex than those shown in Figure 2 lead to the mediation method exhibiting departures from the simulated effects. 

The models tested are shown in Figures 4-8. There is a mixture of different numbers of variables, different model complexities, and simulating effect sizes from non-guassian distributions. A comparison of the deconvolved effect estimates from each of these models against the true simulated effects is shown in Figure 9.

The inversion method performs reliably in most cases, however occasionally in more complex models there will appear to be some direct effects that have non-zero estimates where they should have had no effect. This may be an indication of saturation.



```{r }
l <- list()
dat <- init_dat(500000, 4)
dat <- make_edge(1,2,2, dat)
dat <- make_edge(2,3,3, dat)
dat <- make_edge(2,4,7, dat)
dat <- make_edge(1,3,4, dat)
dat <- make_edge(3,4,5, dat)
dat <- make_edge(1,4,6, dat)

res <- graph_mr(dat)
res1 <- mediation_method(res)
res2 <- inversion_method(res)
res3 <- deconvolution_method(res)
l[[1]] <- rbind(
	data.frame(estimate=c(res1), truth=c(dat$r), method="Mediation"),
	data.frame(estimate=c(res2), truth=c(dat$r), method="Inversion"),
	data.frame(estimate=c(res3), truth=c(dat$r), method="Feizi"),
	data.frame(estimate=c(res$b), truth=c(dat$r), method="Total effects")
)
l[[1]]$model <- "4-variables, complex"
```

```{r, fig.cap="Graphs for '4-variables, comples' model" }

grid.arrange(grobs=list(
	plot_from_matrix_clean(dat$r, "True graph"),
	plot_from_matrix_clean(res1, "Mediation method"),
	plot_from_matrix_clean(res2, "Inversion method"),
	plot_from_matrix_clean(res3, "Feizi method"),
	plot_from_matrix_clean(res$b, "Total effects")
), ncol=2)

```

```{r }

dat <- init_dat(500000, 5)
dat <- make_edge(1,2,2, dat)
dat <- make_edge(2,3,3, dat)
dat <- make_edge(3,4,7, dat)
dat <- make_edge(4,5,4, dat)

res <- graph_mr(dat)
res1 <- mediation_method(res)
res2 <- inversion_method(res)
res3 <- deconvolution_method(res)

l[[2]] <- rbind(
	data.frame(estimate=c(res1), truth=c(dat$r), method="Mediation"),
	data.frame(estimate=c(res2), truth=c(dat$r), method="Inversion"),
	data.frame(estimate=c(res3), truth=c(dat$r), method="Feizi"),
	data.frame(estimate=c(res$b), truth=c(dat$r), method="Total effects")
)
l[[2]]$model <- "5-variables, causal chain"

```

```{r, fig.cap="Graphs for '5-variables, causal chain' model" }

grid.arrange(grobs=list(
	plot_from_matrix_clean(dat$r, "True graph"),
	plot_from_matrix_clean(res1, "Mediation method"),
	plot_from_matrix_clean(res2, "Inversion method"),
	plot_from_matrix_clean(res3, "Feizi method"),
	plot_from_matrix_clean(res$b, "Total effects")
), ncol=2)

```

```{r }

p <- 15
dat <- init_dat(300000, p)
for(i in 1:(p-1))
{
	dat <- make_edge(i, i + 1, rnorm(1), dat)
}

res <- graph_mr(dat)
res2 <- inversion_method(res)
res3 <- deconvolution_method(res)

l[[3]] <- rbind(
	data.frame(estimate=c(res2), truth=c(dat$r), method="Inversion"),
	data.frame(estimate=c(res3), truth=c(dat$r), method="Feizi"),
	data.frame(estimate=c(res$b), truth=c(dat$r), method="Total effects")
)
l[[3]]$model <- "15-variables, causal chain"

```

```{r, fig.cap="Graphs for '15-variables, causal chain' model" }

grid.arrange(grobs=list(
	plot_from_matrix_clean(dat$r, "True graph"),
	plot_from_matrix_clean(res2, "Inversion method"),
	plot_from_matrix_clean(res3, "Feizi method"),
	plot_from_matrix_clean(res$b, "Total effects")
), ncol=2)

```

```{r }

p <- 20
dat <- init_dat(300000, p)
for(i in 1:(p-1))
{
	dat <- make_edge(i, i + 1, rnorm(1), dat)
}
dat <- make_edge(1,7,rnorm(1), dat)
dat <- make_edge(5,9,rnorm(1), dat)
dat <- make_edge(3,12,rnorm(1), dat)
dat <- make_edge(1,8,rnorm(1), dat)

res <- graph_mr(dat)
res2 <- inversion_method(res)
res3 <- deconvolution_method(res)

l[[4]] <- rbind(
	data.frame(estimate=c(res2), truth=c(dat$r), method="Inversion"),
	data.frame(estimate=c(res3), truth=c(dat$r), method="Feizi"),
	data.frame(estimate=c(res$b), truth=c(dat$r), method="Total effects")
)
l[[4]]$model <- "20-variables, complex"


```

```{r, fig.cap="Graphs of '20-variables, complex' model" }

grid.arrange(grobs=list(
	plot_from_matrix_clean(dat$r, "True graph"),
	plot_from_matrix_clean(res2, "Inversion method"),
	plot_from_matrix_clean(res3, "Feizi method"),
	plot_from_matrix_clean(res$b, "Total effects")
), ncol=2)

```

```{r }

p <- 30
dat <- init_dat(300000, p)
for(i in 1:(p-1))
{
	dat <- make_edge(i, i + 1, rexp(1), dat)
}

res <- graph_mr(dat)
res2 <- inversion_method(res)
res3 <- deconvolution_method(res)

l[[5]] <- rbind(
	data.frame(estimate=c(res2), truth=c(dat$r), method="Inversion"),
	data.frame(estimate=c(res3), truth=c(dat$r), method="Feizi"),
	data.frame(estimate=c(res$b), truth=c(dat$r), method="Total effects")
)
l[[5]]$model <- "30-variables, non-gaussian"

```

```{r, fig.cap="Graphs of '30-variables, non-gaussian' model" }

grid.arrange(grobs=list(
	plot_from_matrix_clean(dat$r, "True graph"),
	plot_from_matrix_clean(res2, "Inversion method"),
	plot_from_matrix_clean(res3, "Feizi method"),
	plot_from_matrix_clean(res$b, "Total effects")
), ncol=2)

```

```{r fig.cap="Comparison of deconvolved direct effect estimates from different methods against the true simulated direct effects.", fig.height=9.5 }

L <- bind_rows(l)

ggplot(L, aes(y=pmax(pmin(estimate, 10), -10), x=truth)) +
geom_point(alpha=0.4) +
facet_grid(model ~ method, scale="free") +
labs(x="True (simulated) direct effects", y="Estimated effects for each method")

```


## Statistical power

### Basic simulations

As an illustration, in the following analyses a 6-trait network is created as in figure 1. Is it statistically more efficient to 

1. estimate the total effect of trait 1 on trait 6 using standard MR, or
2. obtain the direct effects of the 6-trait network, and estimate the causal chain from trait `1 -> 2 -> 3 -> 4 -> 5 -> 6`?

Strategy (2) entails finding a 'significant' p-value at each of the 5 direct relationships that map trait 1 to trait 6, whereas strategy (1) simply requires that the total effect of trait 1 on trait 6 is 'significant'. For the purposes of evaluating power we are determining 'significance' at a p < 0.01 threshold, but such practice is not considered reliable when applied to real data [@Sterne2001].

A significant path is found for a hypothesised relationsip of 1 causing 6 as follows. A directed path is searched for that links the two nodes along edges that all have p < 0.01. If any edge doesn't satisfy this threshold then another path needs to be found. This uses a graph traversal algorithm (breadth first search of Dijkstra's algorithm). This means that some paths from 1 to 6 might be found *through the wrong pathway*. But the extent to which this is happening is evaluated in the false discovery rate.

The false discovery rate is estimated as follows. For a graph where a path of 1 causing 6 is simulated, paths are searched for that link **6 causing 5**, i.e. the reverse, for which there should be no paths. This is attempting to be conservative because a) it allows for strong edges to exist in the null model, b) the target node is known to be influenced by at least one other node already.


```{r}
L <- list()

nsim <- 100
for(n in 3:6)
{
	message(n, " pos")
	l <- list()
	for(j in 1:nsim)
	{
		dat1 <- init_dat(5000, n)
		for(i in 1:(n-1))
		{
			dat1 <- make_edge(i, i+1, rnorm(1), dat1)
		}
		res1 <- graph_mr(dat1)
		res1b <- bootstrap_graphs(res1)
		l[[j]] <- test_sig(res1, res1b)
	}
	lpos <- as.data.frame(do.call(rbind, l))
	lpos$n <- n
	lpos$model <- rep(c("Non-null", "Null"), times=nsim)
	L[[n]] <- lpos
}
A <- bind_rows(L)
```

The simulation was performed using 5000 samples, and the simulated direct effect sizes for each chain were sampled from $\beta \sim N(0,1)$. 


```{r, fig.cap="Statistical power comparison between standard MR and graph MR using inversion method. Top graph shows the results from simulations where causal influences were simulated with non-zero effect sizes. Bottom graph shows depicts the false discovery rate, i.e. the same simulation but where all effect sizes are 0." }

AA <- gather(A, key='strategy', value='pval', V1, V2)
AA$strategy <- as.factor(AA$strategy)
levels(AA$strategy) <- c("Total effect", "Chained direct effects")
AAg <- AA %>% group_by(n, model, strategy) %>% summarise(propsig = sum(pval < 0.05)/n())
ggplot(AAg, aes(x=as.factor(n), y=propsig)) +
geom_bar(stat="identity", aes(fill=strategy), position="dodge") +
facet_grid(model ~ .) +
labs(y=paste0("Prop. simulations (out of ", nsim, ") with p < 0.05"), x="Graph size (e.g. number of traits from 1 to n)")
```

Here it is apparent that as the causal chain grows larger the statistical power of the second strategy, which uses the direct effects estimated from the graph, becomes substantially more powerful than the standard approach of identifying the total causal effect. The false discovery rate of the second strategy appears grow as the graph size grows also though.


### Exploring causal graphs with multiple pathways

There are three causal models being evaluated.

- Model 1: One path from 1 to 6
	- `1 -> 2 -> 3 -> 4 -> 5 -> 6`
- Model 2: Two paths from 1 to 6
	- `1 -> 2 -> 3 -> 6`
	- `1 -> 4 -> 5 -> 6`
- Model 3: Three paths from 1 to 6
	- `1 -> 2 -> 6`
	- `1 -> 3 -> 6`
	- `1 -> 4 -> 5 -> 6`
- Null model: No causal effects simulated

```{r}

nsim <- 100


l <- list()
n <- 6
for(j in 1:nsim)
{
	dat1 <- init_dat(5000, n)
	for(i in 1:(n-1))
	{
		dat1 <- make_edge(i, i+1, rnorm(1), dat1)
	}
	res1 <- graph_mr(dat1)
	res1b <- bootstrap_graphs(res1)
	l[[j]] <- test_sig(res1, res1b, 0.01)
}
lpos <- as.data.frame(do.call(rbind, l))
lpos$what <- rep(c("Non-null", "Null"), times=nsim)
lpos$model <- "Model1"

l <- list()
for(j in 1:nsim)
{
	dat1 <- init_dat(5000, n)

	dat1 <- make_edge(1,2,rnorm(1)/3,dat1)
	dat1 <- make_edge(2,3,rnorm(1)/3,dat1)
	dat1 <- make_edge(3,6,rnorm(1)/3,dat1)
	dat1 <- make_edge(1,4,rnorm(1)/3,dat1)
	dat1 <- make_edge(4,5,rnorm(1)/3,dat1)
	dat1 <- make_edge(5,6,rnorm(1)/3,dat1)
	res1 <- graph_mr(dat1)
	res1b <- bootstrap_graphs(res1)
	l[[j]] <- test_sig(res1, res1b, 0.01)
}
l2path <- as.data.frame(do.call(rbind, l))
l2path$what <- rep(c("Non-null", "Null"), times=nsim)
l2path$model <- "Model2"

l <- list()
for(j in 1:nsim)
{
	dat1 <- init_dat(5000, n)

	dat1 <- make_edge(1,2,rnorm(1)/6,dat1)
	dat1 <- make_edge(2,6,rnorm(1)/6,dat1)
	dat1 <- make_edge(1,3,rnorm(1)/6,dat1)
	dat1 <- make_edge(3,6,rnorm(1)/6,dat1)
	dat1 <- make_edge(1,4,rnorm(1)/6,dat1)
	dat1 <- make_edge(4,5,rnorm(1)/6,dat1)
	dat1 <- make_edge(5,6,rnorm(1)/6,dat1)
	res1 <- graph_mr(dat1)
	res1b <- bootstrap_graphs(res1)
	l[[j]] <- test_sig(res1, res1b, 0.01)
}
l3path <- as.data.frame(do.call(rbind, l))
l3path$what <- rep(c("Non-null", "Null"), times=nsim)
l3path$model <- "Model3"

l <- list()
for(j in 1:nsim)
{
	dat1 <- init_dat(5000, n)
	res1 <- graph_mr(dat1)
	res1b <- bootstrap_graphs(res1)
	l[[j]] <- test_sig(res1, res1b, 0.01)
}
lneg <- as.data.frame(do.call(rbind, l))
lneg$what <- "Null"
lneg$model <- "No effects"

B <- bind_rows(list(lpos, l2path, l3path, lneg))

```

The simulation was performed using 5000 samples, and the simulated direct effect sizes for each chain were sampled from $\beta \sim N(0,1)$.

To generate a basic FDR, a different dataset was generated with no simulated causal paths. A graph traversal algorithm is applied to find a path of significant links to make a chain from 1 to 6. This is not constrained to go through 2,3,4,5, it can go through any path to get from 1 to 6. 


```{r, fig.cap="Statistical power comparison between standard MR and graph MR using inversion method. Top graph shows the results from simulations where causal influences were simulated with non-zero effect sizes. Bottom graph shows depicts the false discovery rate, i.e. the same simulation but where all effect sizes are 0." }

BB <- gather(B, key='strategy', value='pval', V1, V2)
AA$strategy <- as.factor(AA$strategy)
levels(AA$strategy) <- c("Total effect", "Chained direct effects")


BBg <- BB %>% group_by(what, model, strategy) %>% summarise(propsig = sum(pval < 0.01) / n())

ggplot(BBg, aes(x=as.factor(model), y=propsig)) +
geom_bar(stat="identity", aes(fill=strategy), position="dodge") +
facet_grid(what ~ .) +
labs(y=paste0("Prop. simulations (out of ", nsim, ") with p < 0.05"), x="Graph size (e.g. number of traits from 1 to n)")

```


### False discovery rates

The number of possible $k$ length paths between two nodes in a graph of size $n$ is given as 

$$
p_n(k) = \frac{(n - 2)!}{(n - k)!}
$$

and the probability of a false discovery for a path of length $k$ is $\alpha^k$ where $\alpha$ here is 0.05. Hence, the probability of finding a path between two nodes is

$$
FDR_{n} = \alpha + \sum^n_{k=2} \alpha^k \frac{(n - 2)!}{(n - k)!}
$$

this can be simplified to be computationally tractable for large graphs. The FDR for a particular path length $K$ is

$$
\begin{aligned}
FDR_{n}(K) & = \alpha^K \prod^K_{k=2} n - k - 1 \\
           & = \alpha^2 \prod^K_{k=2} \alpha(n - k - 1)
\end{aligned}
$$

Hence, the total FDR across all path lengths is


$$
FDR_{n} = \alpha + \alpha^2 + \sum^N_{K=3} \left( \alpha^2 \prod^K_{k=2} \alpha(n - k - 1) \right )
$$

The effect of graph size on the FDR is shown in the top plot of Figure X. This is what happens if any path size is allowed to be searched for. If the maximum path size is limited to 6 (bottom graph) then the saturation is lower.

```{r, fig.cap="Relationship between graph size, alpha value and FDR"}

l <- list()
param <- expand.grid(n = 3:300, thresh=c(0.05, 0.025, 0.01, 0.005, 0.001), maxchain=c("Max path size = N", "Max path size = 6"))
param$maxchainl <- param$n
param$maxchainl[param$maxchain == "Max path size = 6"] <- 6
for(i in 1:nrow(param))
{
	l[[i]] <- calc_number_of_paths(param$n[i], param$thresh[i], min(param$maxchainl[i], param$n[i]))
	l[[i]]$n <- param$n[i]
	l[[i]]$thresh <- param$thresh[i]
	l[[i]]$maxc <- param$maxchain[i]
}


l <- bind_rows(l)
l1 <- group_by(l, n, thresh, maxc) %>%
	summarise(fdr=pmin(1, sum(fdr)))

ggplot(l1, aes(x=n, y=fdr)) +
# geom_point(aes(colour=as.factor(thresh))) +
geom_line(aes(colour=as.factor(thresh))) +
facet_grid(maxc ~ .) +
labs(x="Number of traits in graph", y="False discovery rate for finding a path between any two nodes", colour="Threshold for\nsignificant\nlink in path")

```


What does this saturation look like e.g. for 50 trait graph with $\alpha = 0.05$?

```{r}

m <- subset(l, n %in% c(20, 40, 60, 80, 100) & thresh == 0.05)
ggplot(m, aes(x=k, y=fdr)) +
geom_point(aes(colour=as.factor(n))) +
geom_line(aes(colour=as.factor(n))) +
facet_wrap(~ n, scale="free") +
labs(x="Chain length", y="FDR, not limited to 1", colour="Graph size")

```


An alternative way to check significance of a path is to permute. Here the off diagonal elements of the matrix of p-values are permuted. What this entails is that the same number of strong edges are retained, and then the question is 'with this number of strong edges amongst all nodes, how likely is it to find a path from A to B?'


```{r, eval=FALSE}

param <- expand.grid(
	n = c(6, 10, 20),
	nsim = 1:50,
	thresh = c(0.05, 0.01, 0.001)
)

for(i in 1:nrow(param))
{
	message(i, " of ", nrow(param))
	dat1 <- init_dat(5000, param$n[i])
	dat1 <- make_edge(1,2,rnorm(1)/6,dat1)
	dat1 <- make_edge(2,6,rnorm(1)/6,dat1)
	dat1 <- make_edge(1,3,rnorm(1)/6,dat1)
	dat1 <- make_edge(3,6,rnorm(1)/6,dat1)
	dat1 <- make_edge(1,4,rnorm(1)/6,dat1)
	dat1 <- make_edge(4,5,rnorm(1)/6,dat1)
	dat1 <- make_edge(5,6,rnorm(1)/6,dat1)
	res1 <- graph_mr(dat1)
	res1b <- bootstrap_graphs(res1)

	# path from 1-6?
	param$pval[i] <- find_path(t(res1b$pval), param$thresh[i], 1, 6)
	param$emp_pval[i] <- get_empirical_pval(res1b$pval, param$thresh[i], 1, 6)
}

save(param, file="empirical_pvals.rdata")

```

```{r}

load("empirical_pvals.rdata")
ggplot(subset(param, thresh < 0.05), aes(x=as.factor(n), y=emp_pval)) +
geom_boxplot(aes(fill=as.factor(thresh))) +
labs(x="Graph size", y="Empirical p-value", fill="Link threshold")


```

This looks to be a much more conservative way to test the significance of the link between two nodes. 



### Cycles (non-DAGs)

This looks problematic

```{r }

dat <- init_dat(500000, 3)
dat <- make_edge(1,2,2, dat)
dat <- make_edge(2,3,3, dat)
dat <- make_edge(3,1,7, dat)

dat <- make_edge(4,5,4, dat)
dat <- make_edge(5,3,3, dat)

res <- graph_mr(dat)
res1 <- mediation_method(res)
res2 <- inversion_method(res)
res3 <- deconvolution_method(res)

```



## Discussion

The inversion method appears to be reasonably accurate in a range of scenarios, though not perfect when graphs become larger and relationships more complex. Departures from the true direct effect model are typically small, but exploration of what leads to this are warranted. 


Issues:

- The mediation method might be interpreted too simplisticly here. I think there needs to be recursion beyond the 3 variable example - i.e. the direct effects are a function of indirect effects at the moment, but these indirect effects probably need to be reduced to direct effects also. This would require identifying a path through which to traverse the graph and recursively estimate the direct effects
- These simulations only looked at relatively sparse graphs, and with no loops.
- Need to evaluate much larger graphs, e.g. thousands of traits
- Why isn't the Feizi method working? Need to make sure it works for symmetric correlation graphs
- Haven't evaluated the influence of cycles in the graph - this is hard to simulate, though if a temporal component were introduced then there will be no cycles.
- Graphical lasso may be useful to make the matrix sparse


## References

